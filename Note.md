## I. Neural networks and deep learning
### Introduction
- Image data: CNN - convolution neural network
- Sequence data: Audio, language
- Structured data (presented in table) and unstructured data (audio, image, text)
- [Sigmoid vs activation functions]: for sigmoid function, when the slope -> 0, gradient descent is very low.

### Neural networks basics
- When implementing neural network with logistic regression, it is easier to keep b (intercept) and w(s) as separate parameters.
- Loss function of LR model: L(yhat,y)=-(ylog(yhat) + (1-y)log(1-yhat))  (a *convex* function)
- Cost function: average of loss.
- Forward propagation: Computation Graph: [C1_W2.pdf - page 18]
- Backward progragation: [C1_W2.pdf - page 20]
    - One step backward = Derivative once
        - Calculating dJ/dv
    - Two step backward:
        - dJ/da = (dJ/dv).(dv/da)  (Chain rule)
- When you are writting codes to implement backpropagation, there will be a final output variable that you really care about or want to optimize.
    - Computing dFinalOutputVar/dvar (eg. dJ/da)
    - When coding, that quantity can just be named 'dvar' rather than 'dFinalOutputVar/dvar'
- Logistic Regression gradient descent: [C1_W2.pdf - page 24]
    - da = dL(a,y)/da = -y/a + (1-y)/(1-a)
    - dz = (dL/da).(da/dz) = da.(a(1-a)) = a - y
        - Explanation: https://community.deeplearning.ai/t/derivation-of-dl-dz/165
    - dw1 = x1dz  => w1 := w1 - αdw1
    - dw2 = x2dz  => w2 := w2 - αdw2
    - db = dz     => b := b - αdb
- Gradient descent on m examples of algorithm: [C1_W2.pdf - page 27, page 33, page 38]
    - In this example, 2 for loops are explicitly implemented -> inefficient -> consider *Vectorization*
- Python and vectorization:
    - [Python run time of for loop vs vectorization.jpg]
    - Parallelization instructions - single instruction multiple data (SIMD) instruction
        - If you use bult-in functions such as *np.function* that don't require you explicityly implementing a for loop, it enables Python numpy to take much better advantag of parallelizm to do your computations much faster.
- Whenever possible, avoid explicit for-loops.
- More vectorzation examples: np.exp(), np.log(), np.abs(), np.maximum(), v**2, 1/v
- Replace one for loop of logistic regression derivatives by vectorization: [C1_W2.pdf - page 33]
- Replace all the for loops by vectorization - *Broadcasting in Python*: [C1_W2.pdf - page 35]
    - Backward propagation with vectorization: [C1_W2.pdf - page 38]
- Broadcasting example in Python:
    - division: percentage = 100*A/(cal.reshape(1,4))
    - adding by a constant, adding by a duplicated vector, adding by a duplicated transposed vector: [C1_W2.pdf - page 41, page 42]
- Tips and tricks to avoid bugs related to broadcasting in Python: 
    + Not using rank 1 array (eg: a = np.random.randn(5,)). Instead, defining a matrix/eg. a column vector: a = np.random.randn(5,1) 
        - Or make sure its shape by assert(a.shape==(5,1)) OR a = a.reshape((5,1))
- Logistic regression cost function:
    - Explain lost function [C1_W2.pdf - page 45]
### Overview
- Notation: x(i) for individual observation, x[i] for the order of input layer, a1^[1] node 1 in layer 1.
- Hidden layer: in the training set, the true value for the nodes in the middle are not observed.
    - A network with 1 hidden layer: 2-layer NN (iput layer doesnt count)
- Activation function: 
    - When you have centralized data, may apply tanh instead of sigmoid function. Furthermore, tanh is more superior than sigmoid.
    - For output layer, may apply sigmoid because it is more meaningful when y take value 0, 1.
    - Downside of sigmoid and tanh is when z is very large/small, the gradient descent will be very slow -> may consider ReLU (a = max(0,z))- default, most common choice for activation function.
    - One variation of ReLU: Leaky ReLU (a = max(0.01,z)).
    - If you use linear activation function, no matter how many layer, all the network is doing is just computing a linear activation function without hidden layers.
        - If the problem is linear regression  (eg. predicting temperatures), the output layer can have linear activation function.
- Derivative of sigmoid function: a(1-a)
- Derivative of tanh function: 1-(tanh(z))^2
- Derivative of ReLU function: 0 with z<0, 1 with z>0 -> sub-gradient of the activation function g of z
- Derivative of leaky ReLU function: 0.01 with z<0, 1 with z>0 
- Backpropagation intuition: derivatives and chain rule [C1_W3.pdf - page 30]
- Random initialization: if choosing all the initial weights as zero, the activate functions will be the same.
    - Instead, initialize them randomly: w[1] = np.random.randn((2,2))*0.01 (multiply by a small number so that z will not fall to the tail of sigmoid function)
    - b doesn't have the symmetry breaking problem: b[1] = np.zeros((2,1))
* Learn more about dimension of matrices anc vectors in neural network: https://medium.com/from-the-scratch/deep-learning-deep-guide-for-all-your-matrix-dimensions-and-calculations-415012de1568

### Deep L-layer Neural Networks
- Notation: 
    - n^[l] is the number of unit in layer l 
    - n^[L] = 1 (the last layer - output layer)
    - n^[0] = nx 
    - a^[l]: activation in layer l
- It is okay to have a for loop that goes across layers.
- Dimensions of matrices: (m = # of samples)
    - Z[1] = W[1].X + b[1]
        - Z[1]: (n[1], m)
        - X: (n[0], m)
        - W[1]: (n[1], n[0])
        - b[1]: (n[1], 1)  (will be broadcasted when being added)
    - In backward propagation, dimension of dW should be the same as W: (n[l], n[l-1]), db should be the same as b.
- Circuit theory and deep learning:
    - Informally: there are functions you can compute with a "small" L-layer deep neural network that shallower networks require exponentially more hidden units (neutron?) to compute.
- Forward and backward functions:
    - Storing the cache of z, w, and b is convenient to calculate the derivates later in backward process.
- Forward and backward propagation:
    - Forward: input a[l-1], output a[l], cache z[l] (and w[l], b[l])
    - Backward: input da[l], output da[l-1], dW[l], db[l] (see detail at [C1_W4 pg 17, 18])
- Parameters and hyperparameters:
    - Parameters: Ws, bs
    - Hyperparameters: alpha, iterations, hidden layers, choice of activation function...
    - Rule of thumb: Regularly double check the better value for the hyperparameter

## Improving Deep Neural Networks: Hyperparameter Tuning, Regularization and Optimization
### Practical aspects of deep learning
- Development set: =hold-out cross validation set
- In the past when data has n=100 to 10000, dev and test set accounts 30-40% sample.
- When the data is much larger, dev and test set are not necessarily big. (maybe 2%)
- Problems: mismatched train/test distribution -> make sure they come from the same distribution. 
- Not having a test set might be okay
- Signs when modal is underfitting (high biased): high error on training set, error on test set is not much higher than training set.
- Optimal error/Bayes error: used to justify model error is high or not.
- Basic recipe for Machine Learning:
    - 1) High bias (training data performance) -> Bigger network/Train longer/ Neural network architechture search.
    - 2) High variance (dev set performance) -> More data/ Regularization/ Neural network architechture search.
- In the modern, big data world: improving bias doesn't hurt variance and vice versa. Esp true for neural network
- Regularization:
    - Why do we regularize just w but not b?: w has a lot of parameters, so you aren't fitting all the parameters well whereas b is just a single number.
    - L2 regularization is more common than L1. L1 makes model sparse
    - In the class, lambd was used to not clash with the reserved key in Python
    - L2 repective to neural network: Frobenius norm
    - L2~weight decay
    - Frobenius norm: if set lamba large, some w are nearly zeroed out -> simplify neural network.
- Dropout regularization: assign probability of being eliminated and drop some nodes
    - Implementing dropout ('Inverted dropout'): no matter what you set the threshold (keep.prob) to, this technique ensures the expected value of a[l] remains the same. This makes test time easier because you have less of a scaling problem -> Most common dropout.
- Making predictions at test time: 
    - Not use dropout explicitly, you don't really want your output to be random, it will add noise to your predictions
- Dropout intuition: each node can't rely on any one feature in the previous layer, so have to spread out weights -> tend to shrink weights.
    - Dropout can formally be shown to be an adaptive form of L2 regularization
    - It is alright to vary keep.prob in each layer:
        - Eg. in [C2_W1 page 23], layer 2 have the largest matrix of weight (7x7). To prevent overfitting in this layer, set keep.prob low.
        - Can apply dropout to input layer but not being done offen on practice.
    - Commonly applied for computer vision: the input sizes so big in putting all these pixels that you almost never have enough data. Less frequent in some other areas.
    - Downside: lost function J is no longer well defined on every iteration. 
        - Solution: Graphing J before dropout (turn off it or set keep.prob=1). Then finally applying dropout
- Machine learning comprise:
    - Optimize cosf function J: gradient descent...
    - Not overfit: regularization, get more data ...
-> Orthorganization: think about one task at a time
- Other regularizations: 
    - Data augmentation: reducing the cost of getting new data and overfitting.
    - Early stopping: advantage: don't need to try a log of values of hyperparameters. One downside is not ensuring orthorganization (2 components of machine learning are not independent)
- Normalizing inputs: to speed up training even with small learning rate. Note: use mean and sigma from train set to normalize test set -> ensure both the sets go through the same transformation.
    - Step 1: zero out to the mean
    - Step 2: normalize the variances
- Vanishing/exploding gradients: huge barrier to train deep network
    - If W[l] > Identity matrix, the activations will increase exponentially (explode) and vice versa.
    - Partial solution: carefully choose the initial weights
- Weight initialization: 
    - The larger n (number of input features that go into a neuron), the smaller ws (weights) you want they to be. One way is to set Var(w:)=1/n
        - In practice: w[l] = np.random.randn(shape)*np.sqrt(1/n[l-1])
        - For ReLU, 2/n is better than 1/n
        - In deep-L network, considering n[l-1] instead of n
        - If the input features and activations are roughly mean 0 and standard variance 1, the resulted z tends to take on a similar scale. 
        - This helps reduce the vanishing, exploding
        - For tanh, may use sqrt(1/n) rather 1/n (Xavier initialization)
- Numerical approximation of gradients
    - Two-sided difference (more accurate than one-sided difference): g(theta) ~ (f(theta+epsilon)-f(theta-epsilon))/2epsilon  with error O(epsilon^2)
    - Whereas one-sided difference (f(theta+epsilon)-f(epsilon))/epsilon  with error O(epsilon) -> less accurate
- Gradient checking: find bugs
    - Take all w[l], b[l] and reshape into a big vector 'theta'
    - Take all dw[l], db[l] and reshape into a big vector 'dtheta'
    - Check if dtheta is the gradient of J(theta);
        - for each i: check if dtheta.i_approx = (J(theta.1, theta.2,...,theta.i+epsilon,...) - J(theta.1, theta.2, theta.i-epsilon,...))/2epsilon ~ dtheta.i = dJ/dtheta.i
        - If the difference is 10^-7 -> great, 10^-3 -> worry
- Note on gradient checking:
    - Don't use in training - only to debug
    - If algorithm fails grad check, look at components to try to identify bugs. (look in both db[l] and dw[l])
    - Remember regularization so that dtheta = gradient of J Including the used regularization term.
    - Doesn't work with dropout: can grad check without dropout (keep.prob=1)
    - In some rare situation, implementation of gradient descent is correct when w and b are close to 0: Run at random initialization; perhaps again after some training

### Optimization algorithms
- Mini-batch gradient descent: x{t} : (nx, m/k), y{t} : (1, m/k)
    - Make progress without to wait the process of entire training set.
- One epoch of training: epoch means a single pass through the training set.
- With mini-batch gradient descent, an epoch allows you to take 5000 gradient descent steps.
- Setting mini-batch size =m: batch gradient descent
- Mini-batch size =1: Stochastic gradient descent, can be too noisy
- For small training set <2000: batch gradient descent
- Otherwise typical mini-batch size 64->512: sometimes your code runs faster if mini-batch size is a power of 2.
    - Make sure mini-batch fit in CPU/GPU
- Exponentially weighted averages:
    - Given V_t = beta.V_t-1 + (1 + beta).theta_t
    - V_t as approximated average over 1/(1-beta) days' temperature
    - If V_t is larger, the curve is smoother and shifted to the right
    - Exponetially decaying function: add up to 1, bias correction
    - (1-epsilon)^(1/epsilon) = 1/e  -> 1/epsilon will be the number of days
    - When beta=0.9, you're computing an exponentially weighted average that focuses on just the last 10 days temperature.
    - Advantage: take little memory
    - Bias correction: 
        - When beta is high, the curve starts off really low 
        - Instead of estimating V_t, calculate V_t/(1-beta^t)
- Gradient descent with momentum: works faster than standard gradient descent.
    - V is exponentially weighted average
    - V_dw = beta.V_dw + (1-beta).dw
    - V_db = beta.V_db + (1-beta).db
    - With exponentially weighted average, steps of gradient descent are smoothed out - average values closer to 0
    - Think of dw, db as acceleration, V as velocity and beta as friction when a ball roll down the bowl. The ball gains momentum.
    - 2 hyperparameters: alpha, beta (commonly 0.9)
- RMSprop: root mean square prop - speed up gradient descent
    - Slowdown learning in b direction and speed up learning in w direction
    - S_dw = beta.S_dw + (1-beta).dw^2
    - S_db = beta.S_db + (1-beta).db^2
    - w := w - alpha.dw/sqrt(S_dw)
    - b := b - alpha.db/sqrt(S_db)
    - We want dw is small and db is big
    - When the steps are dampended out, you can choose larger alpha and get faster learning
    - In practice, the directions where you are trying the dampen can be w2, w3, w5...
    - In practice, add small epsilon to sqrt(S_dw) and sqrt(S_db) in the denominator to avoid they are too close to 0.
- Adam optimization algorithm: putting gradient descent with momentum and RMSprop together.
    - Adaptive moment estimation
    - Very effective for many different networks of a very wide variety of architectures.
    - Implementing bias correction
    - w := w - alpha.V_dw_corrected/(sqrt(S_dw_corrected)+epsilon)
    - b := b - alpha.V_db_corrected/(sqrt(S_db_corrected)+epsilon)
    - hyperparameters: alpha - needs to be tuned, beta1 - 0.9, beta2 - 0.999, epsilon - 10^-8 (no need to be tuned)
- Learning rate decay:
    - May help speed up learning algorithm
    - alpha = alpha_0/(1+decay_rate*epoch_num)
    - Other methods: 
        - Exponetinal decay where alpha <1: alpha=alpha_0*0.95^epoch_num
        - alpha0*k/sqrt(epoch_num)
        - Discrete staircase
        - Manual decay: work only on a small number of models
- The problem of local optima:
    - In high dimensional space, you are likely to run into the saddle point -> get stuck in a bad local optima
    - Plateaus: the region that learning is slowed down for a long time
    - Should apply speeding up methods mentioned above.
- Tuning hyperparameters: by importance suggested by Adam:
    - alpha
    - beta, #hidden units, #mini-batch size
    - #layers, learning rate decay
    - default for beta1, beta2, epsilon
    - Don't use a grid. Instead, using random sets of hyperparameters
    - Coarse to find scheme: look for the region where the sets of hyperparameters work best then 'zoom in'.
- Appropriate scale for hyperparameters:
    - alpha: log scale instead of linear scale
        - In Python: r = -4*np.random.randn() <- 