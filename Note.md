## I. Neural networks and deep learning
### Introduction
- Image data: CNN - convolution neural network
- Sequence data: Audio, language
- Structured data (presented in table) and unstructured data (audio, image, text)
- [Sigmoid vs activation functions]: for sigmoid function, when the slope -> 0, gradient descent is very low.

### Neural networks basics
- When implementing neural network with logistic regression, it is easier to keep b (intercept) and w(s) as separate parameters.
- Loss function of LR model: L(yhat,y)=-(ylog(yhat) + (1-y)log(1-yhat))  (a *convex* function)
- Cost function: average of loss.
- Forward propagation: Computation Graph: [C1_W2.pdf - page 18]
- Backward progragation: [C1_W2.pdf - page 20]
    - One step backward = Derivative once
        - Calculating dJ/dv
    - Two step backward:
        - dJ/da = (dJ/dv).(dv/da)  (Chain rule)
- When you are writting codes to implement backpropagation, there will be a final output variable that you really care about or want to optimize.
    - Computing dFinalOutputVar/dvar (eg. dJ/da)
    - When coding, that quantity can just be named 'dvar' rather than 'dFinalOutputVar/dvar'
- Logistic Regression gradient descent: [C1_W2.pdf - page 24]
    - da = dL(a,y)/da = -y/a + (1-y)/(1-a)
    - dz = (dL/da).(da/dz) = da.(a(1-a)) = a - y
        - Explanation: https://community.deeplearning.ai/t/derivation-of-dl-dz/165
    - dw1 = x1dz  => w1 := w1 - αdw1
    - dw2 = x2dz  => w2 := w2 - αdw2
    - db = dz     => b := b - αdb
- Gradient descent on m examples of algorithm: [C1_W2.pdf - page 27, page 33, page 38]
    - In this example, 2 for loops are explicitly implemented -> inefficient -> consider *Vectorization*
- Python and vectorization:
    - [Python run time of for loop vs vectorization.jpg]
    - Parallelization instructions - single instruction multiple data (SIMD) instruction
        - If you use bult-in functions such as *np.function* that don't require you explicityly implementing a for loop, it enables Python numpy to take much better advantag of parallelizm to do your computations much faster.
- Whenever possible, avoid explicit for-loops.
- More vectorzation examples: np.exp(), np.log(), np.abs(), np.maximum(), v**2, 1/v
- Replace one for loop of logistic regression derivatives by vectorization: [C1_W2.pdf - page 33]
- Replace all the for loops by vectorization - *Broadcasting in Python*: [C1_W2.pdf - page 35]
    - Backward propagation with vectorization: [C1_W2.pdf - page 38]
- Broadcasting example in Python:
    - division: percentage = 100*A/(cal.reshape(1,4))
    - adding by a constant, adding by a duplicated vector, adding by a duplicated transposed vector: [C1_W2.pdf - page 41, page 42]
- Tips and tricks to avoid bugs related to broadcasting in Python: 
    + Not using rank 1 array (eg: a = np.random.randn(5,)). Instead, defining a matrix/eg. a column vector: a = np.random.randn(5,1) 
        - Or make sure its shape by assert(a.shape==(5,1)) OR a = a.reshape((5,1))
- Logistic regression cost function:
    - Explain lost function [C1_W2.pdf - page 45]
### Overview
- Notation: x(i) for individual observation, x[i] for the order of input layer, a1^[1] node 1 in layer 1.
- Hidden layer: in the training set, the true value for the nodes in the middle are not observed.
    - A network with 1 hidden layer: 2-layer NN (iput layer doesnt count)
- Activation function: 
    - When you have centralized data, may apply tanh instead of sigmoid function. Furthermore, tanh is more superior than sigmoid.
    - For output layer, may apply sigmoid because it is more meaningful when y take value 0, 1.
    - Downside of sigmoid and tanh is when z is very large/small, the gradient descent will be very slow -> may consider ReLU (a = max(0,z))- default, most common choice for activation function.
    - One variation of ReLU: Leaky ReLU (a = max(0.01,z)).
    - If you use linear activation function, no matter how many layer, all the network is doing is just computing a linear activation function without hidden layers.
        - If the problem is linear regression  (eg. predicting temperatures), the output layer can have linear activation function.
- Derivative of sigmoid function: a(1-a)
- Derivative of tanh function: 1-(tanh(z))^2
- Derivative of ReLU function: 0 with z<0, 1 with z>0 -> sub-gradient of the activation function g of z
- Derivative of leaky ReLU function: 0.01 with z<0, 1 with z>0 
- Backpropagation intuition: derivatives and chain rule [C1_W3.pdf - page 30]
- Random initialization: if choosing all the initial weights as zero, the activate functions will be the same.
    - Instead, initialize them randomly: w[1] = np.random.randn((2,2))*0.01 (multiply by a small number so that z will not fall to the tail of sigmoid function)
    - b doesn't have the symmetry breaking problem: b[1] = np.zeros((2,1))
* Learn more about dimension of matrices anc vectors in neural network: https://medium.com/from-the-scratch/deep-learning-deep-guide-for-all-your-matrix-dimensions-and-calculations-415012de1568

### Deep L-layer Neural Networks
- Notation: 
    - n^[l] is the number of unit in layer l 
    - n^[L] = 1 (the last layer - output layer)
    - n^[0] = nx 
    - a^[l]: activation in layer l
- It is okay to have a for loop that goes across layers.
- Dimensions of matrices: (m = # of samples)
    - Z[1] = W[1].X + b[1]
        - Z[1]: (n[1], m)
        - X: (n[0], m)
        - W[1]: (n[1], n[0])
        - b[1]: (n[1], 1)  (will be broadcasted when being added)
    - In backward propagation, dimension of dW should be the same as W: (n[l], n[l-1]), db should be the same as b.
- Circuit theory and deep learning:
    - Informally: there are functions you can compute with a "small" L-layer deep neural network that shallower networks require exponentially more hidden units (neutron?) to compute.
- Forward and backward functions:
    - Storing the cache of z, w, and b is convenient to calculate the derivates later in backward process.
- Forward and backward propagation:
    - Forward: input a[l-1], output a[l], cache z[l] (and w[l], b[l])
    - Backward: input da[l], output da[l-1], dW[l], db[l] (see detail at [C1_W4 pg 17, 18])
- Parameters and hyperparameters:
    - Parameters: Ws, bs
    - Hyperparameters: alpha, iterations, hidden layers, choice of activation function...
    - Rule of thumb: Regularly double check the better value for the hyperparameter

## Improving Deep Neural Networks: Hyperparameter Tuning, Regularization and Optimization
### Practical aspects of deep learning
- Development set: =hold-out cross validation set
- In the past when data has n=100 to 10000, dev and test set accounts 30-40% sample.
- When the data is much larger, dev and test set are not necessarily big. (maybe 2%)
- Problems: mismatched train/test distribution -> make sure they come from the same distribution. 
- Not having a test set might be okay
- Signs when modal is underfitting (high biased): high error on training set, error on test set is not much higher than training set.
- Optimal error/Bayes error: used to justify model error is high or not.
- Basic recipe for Machine Learning:
    - 1) High bias (training data performance) -> Bigger network/Train longer/ Neural network architechture search.
    - 2) High variance (dev set performance) -> More data/ Regularization/ Neural network architechture search.
- In the modern, big data world: improving bias doesn't hurt variance and vice versa. Esp true for neural network
- Regularization:
    - Why do we regularize just w but not b?: w has a lot of parameters, so you aren't fitting all the parameters well whereas b is just a single number.
    - L2 regularization is more common than L1. L1 makes model sparse
    - In the class, lambd was used to not clash with the reserved key in Python
    - L2 repective to neural network: Frobenius norm
    - L2~weight decay
    - Frobenius norm: if set lamba large, some w are nearly zeroed out -> simplify neural network.
- Dropout regularization: assign probability of being eliminated and drop some nodes
    - Implementing dropout ('Inverted dropout'): no matter what you set the threshold (keep.prob) to, this technique ensures the expected value of a[l] remains the same. This makes test time easier because you have less of a scaling problem -> Most common dropout.
- Making predictions at test time: 
    - Not use dropout explicitly, you don't really want your output to be random, it will add noise to your predictions
- Dropout intuition: each node can't rely on any one feature in the previous layer, so have to spread out weights -> tend to shrink weights.
    - Dropout can formally be shown to be an adaptive form of L2 regularization
    - It is alright to vary keep.prob in each layer:
        - Eg. in [C2_W1 page 23], layer 2 have the largest matrix of weight (7x7). To prevent overfitting in this layer, set keep.prob low.
        - Can apply dropout to input layer but not being done offen on practice.
    - Commonly applied for computer vision: the input sizes so big in putting all these pixels that you almost never have enough data. Less frequent in some other areas.
    - Downside: lost function J is no longer well defined on every iteration. 
        - Solution: Graphing J before dropout (turn off it or set keep.prob=1). Then finally applying dropout
- Machine learning comprise:
    - Optimize cosf function J: gradient descent...
    - Not overfit: regularization, get more data ...
-> Orthorganization: think about one task at a time
- Other regularizations: 
    - Data augmentation: reducing the cost of getting new data and overfitting.
    - Early stopping: advantage: don't need to try a log of values of hyperparameters. One downside is not ensuring orthorganization (2 components of machine learning are not independent)
- Normalizing inputs: to speed up training even with small learning rate. Note: use mean and sigma from train set to normalize test set -> ensure both the sets go through the same transformation.
    - Step 1: zero out to the mean
    - Step 2: normalize the variances
- Vanishing/exploding gradients: huge barrier to train deep network
    - If W[l] > Identity matrix, the activations will increase exponentially (explode) and vice versa.
    - Partial solution: carefully choose the initial weights
- Weight initialization: 
    - The larger n (number of input features that go into a neuron), the smaller ws (weights) you want they to be. One way is to set Var(w:)=1/n
        - In practice: w[l] = np.random.randn(shape)*np.sqrt(1/n[l-1])
        - For ReLU, 2/n is better than 1/n
        - In deep-L network, considering n[l-1] instead of n
        - If the input features and activations are roughly mean 0 and standard variance 1, the resulted z tends to take on a similar scale. 
        - This helps reduce the vanishing, exploding
        - For tanh, may use sqrt(1/n) rather 1/n (Xavier initialization)
- Numerical approximation of gradients
    - Two-sided difference (more accurate than one-sided difference): g(theta) ~ (f(theta+epsilon)-f(theta-epsilon))/2epsilon  with error O(epsilon^2)
    - Whereas one-sided difference (f(theta+epsilon)-f(epsilon))/epsilon  with error O(epsilon) -> less accurate
- Gradient checking: find bugs
    - Take all w[l], b[l] and reshape into a big vector 'theta'
    - Take all dw[l], db[l] and reshape into a big vector 'dtheta'
    - Check if dtheta is the gradient of J(theta);
        - for each i: check if dtheta.i_approx = (J(theta.1, theta.2,...,theta.i+epsilon,...) - J(theta.1, theta.2, theta.i-epsilon,...))/2epsilon ~ dtheta.i = dJ/dtheta.i
        - If the difference is 10^-7 -> great, 10^-3 -> worry
- Note on gradient checking:
    - Don't use in training - only to debug
    - If algorithm fails grad check, look at components to try to identify bugs. (look in both db[l] and dw[l])
    - Remember regularization so that dtheta = gradient of J Including the used regularization term.
    - Doesn't work with dropout: can grad check without dropout (keep.prob=1)
    - In some rare situation, implementation of gradient descent is correct when w and b are close to 0: Run at random initialization; perhaps again after some training

### Optimization algorithms
- Mini-batch gradient descent: x{t} : (nx, m/k), y{t} : (1, m/k)
    - Make progress without to wait the process of entire training set.
- One epoch of training: epoch means a single pass through the training set.
- With mini-batch gradient descent, an epoch allows you to take 5000 gradient descent steps.
- Setting mini-batch size =m: batch gradient descent
- Mini-batch size =1: Stochastic gradient descent, can be too noisy
- For small training set <2000: batch gradient descent
- Otherwise typical mini-batch size 64->512: sometimes your code runs faster if mini-batch size is a power of 2.
    - Make sure mini-batch fit in CPU/GPU
- Exponentially weighted averages:
    - Given V_t = beta.V_t-1 + (1 + beta).theta_t
    - V_t as approximated average over 1/(1-beta) days' temperature
    - If V_t is larger, the curve is smoother and shifted to the right
    - Exponetially decaying function: add up to 1, bias correction
    - (1-epsilon)^(1/epsilon) = 1/e  -> 1/epsilon will be the number of days
    - When beta=0.9, you're computing an exponentially weighted average that focuses on just the last 10 days temperature.
    - Advantage: take little memory
    - Bias correction: 
        - When beta is high, the curve starts off really low 
        - Instead of estimating V_t, calculate V_t/(1-beta^t)
- Gradient descent with momentum: works faster than standard gradient descent.
    - V is exponentially weighted average
    - V_dw = beta.V_dw + (1-beta).dw
    - V_db = beta.V_db + (1-beta).db
    - With exponentially weighted average, steps of gradient descent are smoothed out - average values closer to 0
    - Think of dw, db as acceleration, V as velocity and beta as friction when a ball roll down the bowl. The ball gains momentum.
    - 2 hyperparameters: alpha, beta (commonly 0.9)
- RMSprop: root mean square prop - speed up gradient descent
    - Slowdown learning in b direction and speed up learning in w direction
    - S_dw = beta.S_dw + (1-beta).dw^2
    - S_db = beta.S_db + (1-beta).db^2
    - w := w - alpha.dw/sqrt(S_dw)
    - b := b - alpha.db/sqrt(S_db)
    - We want dw is small and db is big
    - When the steps are dampended out, you can choose larger alpha and get faster learning
    - In practice, the directions where you are trying the dampen can be w2, w3, w5...
    - In practice, add small epsilon to sqrt(S_dw) and sqrt(S_db) in the denominator to avoid they are too close to 0.
- Adam optimization algorithm: putting gradient descent with momentum and RMSprop together.
    - Adaptive moment estimation
    - Very effective for many different networks of a very wide variety of architectures.
    - Implementing bias correction
    - w := w - alpha.V_dw_corrected/(sqrt(S_dw_corrected)+epsilon)
    - b := b - alpha.V_db_corrected/(sqrt(S_db_corrected)+epsilon)
    - hyperparameters: alpha - needs to be tuned, beta1 - 0.9, beta2 - 0.999, epsilon - 10^-8 (no need to be tuned)
- Learning rate decay:
    - May help speed up learning algorithm
    - alpha = alpha_0/(1+decay_rate*epoch_num)
    - Other methods: 
        - Exponetinal decay where alpha <1: alpha=alpha_0*0.95^epoch_num
        - alpha0*k/sqrt(epoch_num)
        - Discrete staircase
        - Manual decay: work only on a small number of models
- The problem of local optima:
    - In high dimensional space, you are likely to run into the saddle point -> get stuck in a bad local optima
    - Plateaus: the region that learning is slowed down for a long time
    - Should apply speeding up methods mentioned above.
- Tuning hyperparameters: by importance suggested by Adam:
    - alpha
    - beta, #hidden units, #mini-batch size
    - #layers, learning rate decay
    - default for beta1, beta2, epsilon
    - Don't use a grid. Instead, using random sets of hyperparameters
    - Coarse to find scheme: look for the region where the sets of hyperparameters work best then 'zoom in'.
- Appropriate scale for hyperparameters:
    - alpha: log scale instead of linear scale
        - In Python: r = -4*np.random.randn() -> alpha = 10^r
    - beta: log scale of (1-beta)^r with from -1 to -3.
- Practice
    - !Intuitions do get stale. Re-evaluated occasionally.
    - Babysitting one model: when don't have enough computer resource (Panda approach)
    - Training many models in parallel (Caviar approach) 
- Please note, as pointed out earlier in Week 1, the normalization should be: X=X/σ 
- Normalizing activations in a network: 
    - Normalizing inputs to speed up learning: (X-mu)/σ 
    - In practice, normalizing z is more often than a
    - Can add epsilon at the denominator of the function of z_norm
    - We don't want the hidden units to always have mean 0 and variance 1:
        - z_tilde = gamma*z(i)_norm + beta   (this beta differs from gradient momentum)
        - gamma and beta are learnable parameters of the model
        - Revert normalization: gamma = sqrt(sigma^2 + epsilon); beta = mu
        - Batch normalization can apply normalization to not only the input layer but also hidden layers
- Fitting batch norm into a neural network
    - Batch norm is applies in multi layers
    - beta[l]:= beta[l] - alpha.dbeta[l], same as gamma
    - Working with mini-batches
    - b[l] will be canceled out
- Why does batch norm work?
    - Similar to normalizing input, but also for hidden units
    - *Covariance shift*: x -> y. If distribution of x changes, need to retrain the model.
    - Weights of deeper in the network more robust to changes of weights of earlier layers. It ensures no matter how the change, the mean and variance of z(s) will remain the same 
        - It allows each layer of the network to learn by itself, a little bit more independently of other layers. -> speed up the learning
    - Slight regularization effect
        - Each mini-batch is scaled by the mean/variance computed on just that mini-batch
        - This adds some noise to the values z[l] -> similar to dropout, it adds some noise to each hidden layer's activations.
    - If you use the big mini-batch size, eg 512, you are reducing this noise and so the regularization effect -> wouldn't really use batch norm as a regularizer
- Batch norm at test time
    - Get Mu and sigma from exponentially weighted average on train set where the average is across the mini batches -> sometimes called the running average
    - Calculate z_norm in test set by the resulted Mu and sigma above
    - Z tilde using beta and gamma from training process
- Softmax regression:
    - Outcome have multiple classes C>2
    - yhat is (C,1)
    - Activation function:
        - temporary variable (normalizing?): t = e^z[L]  (element-wise; z[L] and t : (C,1))
        - a[L] = e^z[L]/(sum of t from i to C)   (a: (C,1)) -> the chance of each class
    - Before, function like sigmoid take a real number and output a real number. The unusual thing of Softmax is it take a vector and outout a vector
- Choosing deep learning frameworks
    - Ease of programming (development and deployment)
    - Running speed
    - Truly open (open source with good governance)
- TensorFlow:
    - Only have to implement forward propagration: using GradientTape
```py
import numpy as np
import tensorflow as tf

w = tf.Variable(0, dtype=tf.float32)
x = np.array((1.0, -10.0, 25.0), dtype=np.float32)
optimizer = tf.keras.optimizers.Adam(0.1)

def training(x, w, optimizer):
    def cost_fn():
        return x[0]*w**2 + x[1]*w + x[2]
    for i in range(1000):
        optimizer.minimize(cost_fn, [w])
    return w
w = training(x, w, optimizer)
```
## Structured Machine Learning projects
### Introduction to ML Strategy
- To improve accuracy:
    - Collect more data
    - Collect more diverse training set
    - Train algorithm longer with gradient descent
    - Try Adam instead of gradient descent
    - Try bigger network 
    - Try smaller network
    - Try dropout
    - Add L2 regularization
    - Change network architechture
        - Activation functions
        - Number of hidden units
        ...
- Orthogonalization
    - Orthogonal means at 90 degrees to each other -> hyperparameters are independent. It is more ideal and to have control over independent hyperparameter.
    - For a supervised learning system to do well, make sure:
        - Fit training set well on cost function ~ human-level performance: bigger network, Adam...
        - Fit dev set well on cost function: Regularization, bigger train set
        - Fit test set well: Bigger dev set
        - Performs well in real world: Change dev set or cost function
- Single Number evaluation metric: sometimes called single row number
    - F1 score: combining Precision and Recall - "harmonic mean"
    - Speed up iteration
- Satisficing and Optimizing metric:
    - Combine accuracy and running time into one metric
    - Optimizing metric - you want to maximize it: eg accuracy
    - Satisficing metric - you want it to be good enough: eg. running time <=100ms, false positive every 24 hour
- Train/dev/test distributions
    - Set up dev set + one single metric
    - Ensuring the sets come from the same distribution: 
        - Randomly shuffle into the sets: dev/test set
    - Choose a dev set and test set to reflect data you expect to get in the future and consider important to do well on.
- Dev/test sets' sizes:
    - In the modern world, with large data (~1mil): train set 98% instead of the rule of thumb 70-30 or 60-20-20.
    - The goal is to set the test set to be big enough to give high confidence in the overall performance of your system: 10k or 100k may be enough.
    - Sometimes it's oka to not have test set, when the dev set is very large so that you think you won't overfit the dev set too badly.
- When to change dev/test sets and metrics?
    - Eg. when an algorithm is doing better on the evaluation metric but also a worse one considering other aspect (wrong picture...) -> the evaluation metric is no longer correctly rank order preferences between algorithms.
    - Change evaluation metric: eg put more error weight on porn pic in image classification
        - Step 1 - Place target: what do you want the metric to do
        - Step 2 - Aim/shoot at target: Tune, put additional weight at the error/cost function...
    - If doing well on metric + dev/test set does not correspond to doing well on your application, change your metric and/pr dev/test set.
- Why human-level performance?
    - Bayes optimal error: best possible error 
    - Accuracy increase often slows down after surpassing human-level performance (hlp)
        - One reason is hlp is not far from Bayes optimal error
        - If performance is under hlp, there are tools to easily improve it.
    - In tasks humans are good at (or better than ML):
        - Get labeled data from humans.
        - Gain insight from manual error analyssi: why did a person get this right?
        - Better analysis of bias/variance
- Avoidable bias:
    - For some situations, think of hlp as a proxy for Bayes error.
    - Eg:
        - If hlp 1%, training error 8%, dev error 10% -> reduce bias
        - If hlp 7.5%, training error 8%, dev error 10% -> reduce variance
    - Avoidable bias: the distance between training error and hlp
    - Your can't actually do better than Bayes error unless you're overfitting.
- Understanding human-level performance:
    - Be clear of the objective to define hlp
    - Different definition of hlp can lead to different values -> may change tactic for reducing avoidable bias or variance, esp when you're doing really well on the algorithm.
- Surpass hlp:
    - Problems where ML significantly surpasses hlp:
        - Online advertising, product recommendations, logistics (predicting transit time), loan approvals -> huge amount of structured data, not natural perception problems.
        - Even speech recognition,, some image recognition tasks...
- Improving model performance:
    - Assumptions of supervised learning:
        1. Fit the training set pretty well: low avoidable bias
        2. Generalizes pretty well to the dev/test set: variance
    - Reducing bias and variance: choosing tactic as mentioned about
        - Reducing bias: bigger model, longer/better optimizaiton algo, NN architechture/hyperparameter search.
        - Reducing variance: more data, regularization, NN architechture/hyperparameter search.
- Error analysis
    - Eg: To decide for a cat classifier if we should improve it so that it will not mistake dog: Get ~100 mislabeled dev set examples and Count up how many are dogs. 
    - Upper bound on how much you could improve performance by working on the dog problem -> ceiling on performance
    - Evaluate multiple ideas in parallel: eg. pictures of dogs, great cats, blurry images
    - Recode in spreadsheet: [C3W2 pg 4]
- Cleaning up incorrectly labeled data
    - Deep learning algorithms are quite robust to random errors in the training set: so long those erros are not too far from random
    - If those incorrectnesses in dev/test set: during error analysis, add one column for counting up when label Y was incorrect -> find examples where the classifier disagrees with the label in the dev set.
    - If the errors makes a significant difference (considering the fraction of errors due to incorrect labels/Overall dev set error) to the ability to evaluate algorithms on your dev set -> go ahead and spend time to fix incorrect labels.
    - Remember the goal of dev set is to help you select between two classifiers A and B: if this selection is affected by incorrect labeling, go fix the problem.
    - Correcting incorrect dev/test set examples: 
        - Apply same process to your dev and test sets to makes sure they continue to come from the same distribution.
        - Consider examining exa ples your algorithm got right as well as ones it got wrong -> not easy to do
        - Tran and dev/test data may now come from slightly different distribution (in case dev/test set are corrected by train set isn't).
- Build your first system quickly, then iterate: 
    - Set up dev/test set and metric
    - Build initial system quickly: find the train set, train it and see
    - Bias/Variance analysis and Error analysis to prioritize next steps.
    - This advice applies less strongly if you are working on an application area in which you have significant prior experience, or there is a siginificant body of academic literature having pretty much the exact same problem you are building. 
- Training and testing on different distributions: 
    - In case you need many data that not only come from the distribution you are concerning
    - Option 1: combine the sources of data, shuffle then dividing train/dev/test set -> against this because dev set doesn't reflect the distribution you concern
    - Option 2: divide the sets so that dev and test set include only the distribution you concern -> aiming at the target
-  Bias and Variance with Mismatched data distributions
    - Making training-dev set: further split train set into this set. This set isn't used for training model, but for estimating the true error due to variance problem. Otherwise is data mismatch problem
    - General principles: to detect bias/varirance/or data mismatch problem
    1. Human level
    2. Training set error
    3. Train-dev set error
    4. Dev set error
    5. (Maybe) Test set error: the degree if overfitting to the dev set
    - More general formulation: [C3_W2.pdf pg 17]
- Multi-task learning (MTL)
    - Unline softmax: each intance can have multiple label.
    - If some of the earlier features in neural network can be shared between the different types of objects (labels), training One neural network to do multiple things results in better performance than training completely separate networks to do each task.
    - MTL also works even if labels are not fully recorded in one instance sometimes (missing): just sum up only value of 1 and 0.
    - When MTL makes sense:
        - Training on a set of tasks that could benefit from having shared lower-level features.
        - Usually: amount of data you have for each task is quite similar.
        - Can train a big enough neural network to do well on all the tasks
- End-to-end learning:
    - With small datasets, the traditional pipelines work better and vice versa
    - With medium datasets, may consider bypassing some steps
    - If you don't have enough data, breaking the problem down to sub-problems may results in better performance than a pure end-to-end deep learning approach.
- When to use end-to-end:
+ Based on experience